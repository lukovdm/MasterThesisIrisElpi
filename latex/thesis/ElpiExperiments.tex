\documentclass[thesis.tex]{subfiles}

\begin{document}

\chapter{Elpi introduction}
In this chapter we will show how \elpi together with \ce can be used to create new tactics. We will do this by giving a tutorial on how to implement the \coqinline{iIntros} tactic from \iris.

We implement our tactic in the $\lambda$Prolog language \elpi \cite{dunchevELPIFastEmbeddable2015,guidiImplementingTypeTheory2019}. \elpi implements $\lambda$prolog \cite{millerHigherorderLogicProgramming1986,millerUniformProofsFoundation1991,belleanneePragmaticReconstructionLProlog1999,millerProgrammingHigherOrderLogic2012} and adds constraint handling rules to it \cite{monfroyConstraintHandlingRules2011}. constraint handling will be explained in Section ?.
\todoo{Defer constraint handling to later}

To use \elpi as a \coq meta programming language, there exists the \elpi \coq connector, \ce \cite{tassiElpiExtensionLanguage2018}. We will use \ce to implement the \elpi variant of \coqinline{iIntros}, named above as \coqinline{eiIntros}.

Our \elpi implementation \coqinline{eiIntros} consists of three parts as seen in \cref{fig:eiintrosstruct}. The first two parts will interpret the DSL used to describe what we want to introduce. Then, the last part will apply the interpreted DSL. In \cref{ssec:tokenizer} we describe how a string is tokenized by the tokenizer. In \cref{ssec:parser} we describe how a list of tokens is parsed into a list of intro patterns. In \cref{ssec:applier} we describe how we use an intro pattern to introduce and eliminate the needed connectives. In every section we describe more parts of the \elpi programming language and the \ce connector starting with the base concepts of the language and working up to the mayor concepts of \elpi and \ce.
\begin{figure}
  \centering
  \begin{tikzpicture}[
      node distance=1cm and 2cm,
      >=stealth,
      auto
    ]
    \node[] (s) {};
    \node[basic box = green, below=of s] (t) {Tokenizer};
    \node[basic box = green, below=of t] (p) {Parser};
    \node[basic box = blue, below=of p] (id) {Introducer \& Destructor};
    \node[left=of id] (g) {};
    \node[below=of id] (e) {};

    \path[->,thick]
    (s) edge node[right]{\elpiinline{string}} (t)
    (t) edge node[right]{\elpiinline{list token}} (p)
    (p) edge node[right]{\elpiinline{list intro_pat}} (id)
    (g) edge node[above]{\elpiinline{goal}} (id)
    (id) edge node[right]{\elpiinline{proof, list goal}} (e);
  \end{tikzpicture}
  \caption{Structure of \coqinline{eiIntros} with the input and output types on the edges.}
  \label{fig:eiintrosstruct}
\end{figure}
%Zeg dat ook het doel is om een tutorial te geven over elpi

\section{Tokenizer}\label{ssec:tokenizer}
The tokenizer takes as input a string. We will interpret every symbol in the string and produce a list of tokens from this string. Thus, the first step is to define our tokens. Next we show how to define a predicate that transform our string into the tokens we defined.

\subsection{Data types}\label{sssec:datatypes}
We have separated the introduction patterns into several distinct tokens. Most tokens just represent one or two characters, but some tokens also contain some data associated with that token. For example \elpiinline{"H1"} is tokenized as the name token containing the string "H1".
\begin{elpicode}
  kind token type.

  type tAnon, tFrame, tBar, tBracketL, tBracketR, tAmp,
       tParenL, tParenR, tBraceL, tBraceR, tSimpl,
       tDone, tForall, tAll token.
  type tName string -> token.
  type tNat int -> token.
  type tPure option string -> token.
  type tArrow direction -> token.

  kind direction type.
  type left, right direction.
\end{elpicode}
We first define a new type called token using the \elpiinline{kind} keyword, where \elpiinline{type} specifies the kind of our new type. Then we define several constructors for the token type. These constructors are defined using the \elpiinline{type} keyword, we specify a list of names for the constructors and then the type of those constructors. The first set of constructors do not take any arguments, thus have type \elpiinline{token}, and just represent one or more constant characters. The next few constructors take an argument and produce a token, thus allowing us to store data in the tokens. For example, \elpiinline{tName} has type \elpiinline{string -> token}, thus containing a string. Besides \elpiinline{string}, there are a few more basic types in \elpi such as \elpiinline{int}, \elpiinline{float} and \elpiinline{bool}. We also have higher order types, like \elpiinline{option A}, and later on \elpiinline{list A}.
\begin{elpicode}
  kind option type -> type.
  type none option A.
  type some A -> option A.
\end{elpicode}
Creating types of kind \elpiinline{type -> type} can be done using the \elpiinline{kind} directive and passing in a more complicated kind.

We can now represent a string as a list of these tokens. Given the string \elpiinline{"[H %H']"} we can represent it as the following list of type \elpiinline{token}: 
\begin{elpicode}
  [tBracketL, tName "H", tPure (some "H'"), tBracketR]
\end{elpicode}

\subsection{Predicates} \label{sssec:predicates}
Programs in \elpi consist of predicates. Every predicate can have several rules to describe the relation between its inputs and outputs.
\begin{elpicode}
  pred tokenize i:string, o:list token.
  tokenize S O :- 
    rex.split "" S SS,
    tokenize.rec SS O.
\end{elpicode}
Line 1 describes the type of the predicate. The keyword \elpiinline{pred} starts the definition of a predicate. Next we give the name of the predicate, "tokenize". Lastly, we give a list of arguments of our predicate. Each argument is marked as either \elpiinline{i:}, they act as an input or \elpiinline{o:}, they act as an output, in \cref*{sssec:mandu} a more precise definition is given. In the only rule of our predicate, defined on line 2, we assign a variable to both of the arguments. \elpiinline{S} has type \elpiinline{string} and is bound to the first argument. \elpiinline{O} has type \elpiinline{list token} and is bound to the second argument. By calling predicates after the \elpiinline{:-} symbol we can define the relation between the arguments. The first predicate we call, \elpiinline{rex.split}, has the following type:
\begin{elpicode}
  pred rex.split i:string, i:string, o:list string.
\end{elpicode}
When we call it, we assign the empty string to its first argument, the string we want to tokenize to the second argument, and we store the output list of string in the new variable \elpiinline{SS}. This predicate allows us to split a string at a certain delimiter. We take as delimiter the empty string, thus splitting the string up in a list of strings of one character each. Strings in \elpi are based on OCaml strings and are not lists of characters. Since \elpi does not support pattern matching on partial strings, we need this workaround.

The next line, line 4, calls the recursive tokenizer, \elpiinline{tokenizer.rec}\footnote{Names in \elpi can have special characters in them like \elpiinline{.}, \elpiinline{-} and \elpiinline{>}, thus, \elpiinline{tokenize} and \elpiinline{tokenize.rec} are fully separate predicates. It is just a convention that when creating a helper predicate we name it by adding a dot and a short name for the helper.}, on the list of split string and assigns the output to the output variable \elpiinline{O}.

The reason predicates in \elpi are called predicates and not functions, is that they don't always have to take an input and give an output. They can sometimes better be seen as predicates defining for which values of their arguments they hold. Each rule defines a list of predicates that need to hold for their premise to hold. Thus, a predicate can have multiple values for its output, as long as they hold for all contained rules. These multiple possible values can be reached by backtracking, which we will discuss in \cref*{sssec:backtracking}. To execute a predicate, we thus find the first rule for which its premise is sufficient for the arguments we supply. We then check if each of the predicates in the conclusion hold starting at the top. If they hold, and we get a value for every output argument, we are done executing our predicate. How we determine when arguments are sufficient and what happens when a rule does not hold, we will discuss in the next two sections.

\subsection{Matching and unification}\label{sssec:mandu}
The arguments of a predicate can be more than just a variable. We can supply a value containing variables and depending on the argument mode, input or output, we match or unify the input with the premise respectively.

\elpiinline{tokenize.rec} uses matching and unification to solve most cases.
\begin{elpicode}
  pred tokenize.rec i:list string, o:list token.
  tokenize.rec [] [] :- !.
  tokenize.rec [" " | SL] TS :- !, tokenize.rec SL TS.
  tokenize.rec ["$" | SL] [tFrame | TS] :- !, 
    tokenize.rec SL TS.
  tokenize.rec ["/", "/", "=" | SL] [tSimpl, tDone | TS] :- !, 
    tokenize.rec SL TS.
  tokenize.rec ["/", "/" | SL] [tDone | TS] :- !, 
    tokenize.rec SL TS.
\end{elpicode}
This predicate has several rules, we chose a few to highlight here. The first rule, on line 2, has a premise and a cut as its conclusion, we will discuss cuts in \cref*{sssec:backtracking}, for now they can be ignored. This rule can be used when the first argument matches \elpiinline{[]} and if the second argument unifies with \elpiinline{[]}. The difference is that, for two values to match they must have the exact same constructors and can only contain variables in the same places in the value. Thus, the only valid value for the first argument of the first rule is \elpiinline{[]}. When unifying two values we allow a variable to be unified with a constructor, when this happens the variable will get assigned the value of the constructor. Thus, we can either pass \elpiinline{[]} to the second argument, or some variable \elpiinline{V}. After the execution of the rule the variable \elpiinline{V} will have the value \elpiinline{[]}.

The next four rules use the same principle. They use the list pattern \elpiinline{[E1, ..., En | TL]}, where \elpiinline{E1} to \elpiinline{En} are the first $n$ values and \elpiinline{TL} is the rest of the list, to match on the first few elements of the list. We unify the output with a list starting with the token that corresponds to the string we match on. The tails of the input and output we pass to the recursive call of the predicate to solve.

When we encounter multiple rules that all match the arguments of a rule we try the first one first. The rules on line 6 and 8 would both match the value \elpiinline{["/", "/", "="]} as first argument. But, we interpret this use the rule on line 6 since it is before the rule on line 8. This results in our list of strings being tokenized as \elpiinline{[tSimpl, tDone]}.

A fun side effect of output being just variables we pass to a predicate is that we can also easily create a function that is reversible. If we change the mode of our first argument to output and move rule 3 to the bottom, we can pass in a list of tokens and get back a list of strings representing this list of tokens.
\quest{Don't know what to do with this, but is an interresting fact and shows the versitility, we might use it later.}

\subsection{Functional programming in \elpi}
While our language is based on predicates we still often defer to a functional style of programming. The first language feature that is very useful for this goal is spilling. Spilling allows us to write the entry point of the tokenizer as defined in \cref*{sssec:predicates} without the need of the temporary variable to pass the list of strings around.
\begin{elpicode}
  pred tokenize i:string, o:list token.
  tokenize S O :- tokenize.rec {rex.split "" S} O.
\end{elpicode}

We spill the output of a predicate into the input of another predicate by using the \elpiinline{{ }} syntax. We don't specify the last argument of the predicate and only the last argument of a predicate can be spilled. It is mostly equal to the previous version, but just written shorter. There is one caveat but it will be discussed in ?.
\todoo{Refer to relevant section}

The second useful feature is how lambda expressions are first class citizens of the language. A \elpiinline{pred} statement is a wrapper around a constructor definition using the keyword \elpiinline{type}, where all arguments are in output mode. The following predicate is equal to the type definition below it.
\begin{elpicode}
  pred tokenize i:string, o:list token.
  type tokenize string -> list token -> prop.
\end{elpicode}
The \elpiinline{prop} type is the type of propositions, and with arguments they become predicates. We are thus able to write predicates that accept other predicates as arguments.
\begin{elpicode}
  pred map i:list A, i:(A -> B -> prop), o:list B.
  map [] _ [].
  map [X|XS] F [Y|YS] :- F X Y, map XS F YS.
\end{elpicode}
\elpiinline{map} takes as its second argument a predicate on \elpiinline{A} and \elpiinline{B}. On line 3 we map this predicate to the variable \elpiinline{F}, and we then use it to either find a \elpiinline{Y} such that \elpiinline{F X Y} holds, or check if for a given \elpiinline{Y}, \elpiinline{F X Y} holds. We can use the same strategy to implement many of the common functional programming higher order functions.

\subsection{Backtracking} \label{sssec:backtracking}
In this section we will finally describe what happens when a rule fails to complete halfway through. We start with a predicate which will be of much use for the last part of our tokenizer.
\begin{elpicode}
  pred take-while-split i:list A, i:(A -> prop), 
                        o:list A, o:list A.
  take-while-split [X|XS] Pred [X|YS] ZS :- Pred X,
    take-while-split XS Pred YS ZS.
  take-while-split XS _ [] XS.
\end{elpicode}
\elpiinline{take-while-split} is a predicate that should take elements of its input list till its input predicate no longer holds and then output the first part of input in its third argument and the last part of the input in its fourth argument.

The predicate contains two rules. The first rule, defined on lines 2 and 3, recurses as long as the input predicate, \elpiinline{Pred} holds for the input list, \elpiinline{[X|XS]}. The second rule returns the last part of the list as soon as \elpiinline{Pred} no longer holds.

The first rule destructs the input in its head \elpiinline{X} and its tail \elpiinline{XS}. It then checks if \elpiinline{Pred} holds for \elpiinline{X}, if it does, we continue the rule and call \elpiinline{take-while-split} on the tail while assigning X as the first element of the first output list and the output of the recursive call as the tail of the first output and the second output. However, if \elpiinline{Pred X} does not succeed we backtrack to the previous rule in our conclusion. Since there is no previous rule in the conclusion we instead undo any unification that has happened and try the next possible rule. This will be the rule on line 4 and returns the input as the second output of the predicate.

We can use \elpiinline{take-while-split} to define the rule for the token \elpiinline{tName}
\begin{elpicode}
  type tName string -> token.

  tokenize.rec SL [tName S | TS] :-
    take-while-split SL is-identifier S' SL',
    { std.length S' } > 0, !,
    std.string.concat "" S' S,
    tokenize.rec SL' TS.
\end{elpicode}
To tokenize a name we first call \elpiinline{take-while-split} with as predicate \elpiinline{is-identifier}, which checks if a string is valid identifier character, wether it is either a letter or one of a few symbols allowed in identifiers. It thus splits up the input string list into an list of string that is a valid identifier and the rest of the input.
On line 5 we check if the length of the identifier is larger than 0. We do this by spilling the length of \elpiinline{S'} into the \elpiinline{>} predicate.
Next, on line 6, we concatenate the list of strings into one string, which will be our name.
And on line 7, we call the tokenizer on the rest of the input, to create the rest of our tokens.

If our length check does not succeed we backtrack to next rule that matches, which is
\begin{elpicode}
  tokenize.rec XS _ :- !, 
    coq.say "unrecognized tokens" XS, fail.  
\end{elpicode}
It prints an error messages saying that the input was not recognized as a valid token, after which it fails. The predicate thus does not succeed. There is one problem, if line 6 or 7 fails for some reason in the \elpiinline{tName} rule of the tokenizer, the current input starting at \elpiinline{X} is not unrecognized as we managed to find a token for the name at the start of the input. Thus, we don't want to backtrack to another rule of \elpiinline{tokenize.rec} when we have found a valid name token. This is where the cut symbol, \elpiinline{!}, comes in. It cuts the backtracking and makes certain that if we fail beyond that point we don't backtrack in this predicate.

If we take the following example
\begin{elpicode}
  tokenize.rec ["H","^"] TS
              ~$\Downarrow \text{calls}$~ 
  tokenize.rec ["^"] TS'
\end{elpicode}
When evaluating this predicate we would first apply the name rule of the \elpiinline{tokenize.rec} predicate. This would unify \elpiinline{TS} with \elpiinline{[tName "H" | TS']} and call line 3, \elpiinline{tokenize.rec ["^"] TS'}. Every rule of \elpiinline{tokenize.rec} fails including the last fail rule. This rule does first print \elpiinline{"unrecognized tokens ^"} but then also fails. Now when executing the rule of line 1, we have failed on the last predicate of the rule. If there was no cut before it, we would backtrack to the fail rule and also print \elpiinline{"unrecognized tokens [H, ^]"}. But, because there is a cut we don't print the faulty error message. Thus, we only print meaningful error message when we fail to tokenize an input.

\section{Parser}\label{ssec:parser}

\subsection{Data structure}
Explain the data structure of intro patterns
\begin{itemize}
  \item Tree structure
  \item iList, allow for destructing of more than just or, and.
\end{itemize}

\begin{elpicode}
  kind ident type.
  type iNamed string -> ident.
  type iAnon term -> ident.

  kind intro_pat type.
  type iFresh, iSimpl, iDone intro_pat.
  type iIdent ident -> intro_pat.
  type iList list (list intro_pat) -> intro_pat.
\end{elpicode}

\subsection{Reductive descent parsing}
\begin{itemize}
  \item We can translate a grammar directly to a parser
  \item Give partial grammar for the intro patterns (in terms of tokens?)
  \item Show code for parser, use \elpiinline{parse_ilist} as example
  \item How to translate a grammar rule to a parser rule
\end{itemize}

\begin{grammar}
  <intropattern\_list> ::= $\epsilon$
  \alt <intropattern> <intropattern\_list>

  <intropattern> ::= <ident>
  \alt `?' | `/=' | `//'
  \alt `[' <intropattern\_list> `]'
  \alt `(' <intropattern\_conj\_list> `)'

  <intropattern\_list> ::= $\epsilon$
  \alt <intropattern> `|' <intropattern\_list>
  \alt <intropattern> <intropattern\_list>

  <intropattern\_conj\_list> ::= $\epsilon$
  \alt <intropattern> `&' <intropattern\_conj\_list>
\end{grammar}

\begin{elpicode}
  pred parse_ip i:list token, o:list token, o:intro_pat.
  parse_ip [tAnon | TS] TS (iFresh) :- !.
  parse_ip [tSimpl | TS] TS (iSimpl) :- !.
  parse_ip [tDone | TS] TS (iDone) :- !.
  parse_ip [tName X | TS] TS (iIdent (iNamed X)) :- !.
\end{elpicode}

\begin{elpicode}
  parse_ip [tBracketL | TS] TS' (iList L) :- !,
  parse_ilist TS [tBracketR | TS'] L.
  parse_ip [tParenL | TS] TS' IP :- !,
  parse_conj_ilist TS [tParenR | TS'] L',
  {std.length L'} >= 2,
  foldr {std.drop-last 2 L'} (iList [{std.take-last 2 L'}]) (x\ a\ r\ r = iList [[x, a]]) IP.
\end{elpicode}

\begin{elpicode}
  pred parse_ilist i:list token, o:list token, o:list (list intro_pat).
  parse_ilist [tBracketR | TS] [tBracketR | TS] [[]].
  parse_ilist TS R [[IP] | LL'] :-
    parse_ip TS [tBar | RT] IP,
    parse_ilist RT R LL'.
  parse_ilist TS R [[IP | L] | LL'] :-
    parse_ip TS RT IP,
    parse_ilist RT R [L | LL'].

  pred parse_conj_ilist i:list token, o:list token, o:list intro_pat.
  parse_conj_ilist TS [tParenR | R] [IP] :- 
    parse_ip TS [tParenR | R] IP.
  parse_conj_ilist TS R [IP | L'] :-
    parse_ip TS [tAmp | RT] IP,
    parse_conj_ilist RT R L'.
\end{elpicode}

\subsection{Danger of backtracking}
Explain how backtracking can lead to massive slowdowns when not managed correctly.
\begin{itemize}
  \item Show timing of current \elpiinline{parse_ilist} code on larger inputs
  \item Change backtracking
  \item Show new timings
\end{itemize}

\begin{elpicode}
  pred parse_ilist i:list token, o:list token, o:list (list intro_pat).
  parse_ilist [tBracketR | TS] [tBracketR | TS] [[]].
  parse_ilist TS R [IPS | LL'] :-
    parse_ip TS RT IP,
    (
      (
        RT = [tBar | RT'],
        parse_ilist RT' R LL',
        IPS = [IP]
      );
      (
        parse_ilist RT R [L | LL'],
        IPS = [IP | L]
      )
    ).
\end{elpicode}

\section{Applier}\label{ssec:applier}
Something about \ce becoming relevant here
\subsection{Elpi coq HOAS}
\coqinline{1+1}
\begin{elpicode}
  app [global (const «Nat.add»), 
      app [global (indc «S»), global (indc «O»)], 
      app [global (indc «S»), global (indc «O»)]]
\end{elpicode}

\coqinline{forall (n: nat), n + 1}
\begin{elpicode}
  prod `n` (global (indt «nat»)) c0 \ 
      app [global (indt «sum»), 
           c0, 
           app [global (indc «S»), global (indc «O»)]]
\end{elpicode}

\subsection{Quotation and anti-quotation}
\begin{elpicode}
  {{ forall (n: nat), n + 1 }} =
    prod `n` (global (indt «nat»)) c0 \ 
        app [global (indt «sum»), 
            c0, 
            app [global (indc «S»), global (indc «O»)]]
\end{elpicode}

\begin{elpicode}
  Plus = {{ 2 }},
  Foo = {{ forall (n: nat), n + lp:Plus }}
\end{elpicode}
\subsection{Coq context in elpi}
\subsection{Proofs in Elpi}
\begin{elpicode}
  pred do-iStartProof i:hole, o:ihole.
  do-iStartProof (hole {{ let _ := _ in _ }} _) _ :- !,
    coq.error "iStartProof: goal is a `let`, use `simpl`, `intros x`, `iIntros (x)`, or `iIntros ""%x""".
  do-iStartProof (hole {{ @envs_entails lp:PROP (@Envs lp:PROPE lp:CI lp:CS lp:N) lp:P }} Proof) (ihole N (hole {{ @envs_entails lp:PROP (@Envs lp:PROPE lp:CI lp:CS lp:N) lp:P }} Proof)) :- !.
  do-iStartProof (hole Type Proof) (ihole N (hole NType NProof)) :- 
    coq.elaborate-skeleton {{ as_emp_valid_2 lp:Type _ (tac_start _ _) }} Type Proof ok,
    Proof = {{ as_emp_valid_2 _ _ (tac_start _ lp:NProof) }},
    coq.typecheck NProof NType ok,
    NType = {{ envs_entails (Envs _ _ lp:N) _}}.
\end{elpicode}

\subsection{Introducing anonymous Iris hypothesis}
\begin{elpicode}
  pred do-iIntros i:(list intro_pat), i:ihole, i:(ihole -> prop).
  do-iIntros [] IH C :- !, if-debug (coq.say "do-iIntros done"), C IH.
  do-iIntros [iFresh | IPS] (ihole N H) C :- !,
    increase-ctx-count N NS,
    do-iIntro-ident (iAnon N) (ihole NS H) IH, !, 
    do-iIntros IPS IH C.
\end{elpicode}

\subsection{Continuation Passing Style}
\begin{elpicode}
  pred do-intro-anon i:hole, i:(hole -> prop).
  do-intro-anon (hole Type Proof) C :-
    coq.ltac.fresh-id "a" {{ False }} ID,
    coq.id->name ID N,
    coq.elaborate-skeleton (fun N _ _) Type Proof ok,
    Proof = (fun _ T IntroFProof),
    @pi-decl N T x\ 
      coq.typecheck (IntroFProof x) (F x) ok,
      C (hole (F x) (IntroFProof x)).
\end{elpicode}

\subsection{Backtracking in proofs}
\begin{elpicode}
  pred do-iIntro-ident i:ident, i:ihole, o:ihole.
  do-iIntro-ident ID (ihole N (hole Type Proof)) (ihole N (hole IType IntroPIProofroof)) :-
    ident->term ID _ T,
    coq.elaborate-skeleton {{ tac_impl_intro _ lp:T _ _ _ _ _ _ _ }} Type Proof ok, !,
    Proof = {{ tac_impl_intro _ _ _ _ _ _ _ _ lp:IProof }},
    coq.typecheck IProof IType' ok,
    pm-reduce IType' IType,
    if (IType = {{ False }}) (coq.error "eiIntro: " X " not fresh") (true).
  do-iIntro-ident ID (ihole N (hole Type Proof)) (ihole N (hole IType IntroPIProofroof)) :-
    ident->term ID _ T,
    coq.elaborate-skeleton {{ tac_wand_intro _ lp:T _ _ _ _ _ }} Type Proof ok, !,
    Proof = {{ tac_wand_intro _ _ _ _ _ _ lp:IProof }}, 
    coq.typecheck IProof IType' ok,
    pm-reduce IType' IType,
    if (IType = {{ False }}) (coq.error "eiIntro: " X " not fresh") (true).
  do-iIntro-ident ID _ _ :-
    ident->term ID X _,
    coq.error "eiIntro: " X " could not introduce".
\end{elpicode}

\end{document}