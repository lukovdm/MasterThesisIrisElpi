\documentclass[thesis.tex]{subfiles}

\begin{document}

\chapter{Elpi introduction}
In this chapter we will show how \elpi together with \ce can be used to create new tactics. We will do this by giving a tutorial on how to implement the \coqi{iIntros} tactic from \iris.

We implement our tactic in the $\lambda$Prolog language \elpi \cite{dunchevELPIFastEmbeddable2015,guidiImplementingTypeTheory2019}. \elpi implements $\lambda$prolog \cite{millerHigherorderLogicProgramming1986,millerUniformProofsFoundation1991,belleanneePragmaticReconstructionLProlog1999,millerProgrammingHigherOrderLogic2012} and adds constraint handling rules to it \cite{monfroyConstraintHandlingRules2011}. constraint handling will be explained in Section ?.
\todoo{Defer constraint handling to later}

To use \elpi as a \coq meta programming language, there exists the \elpi \coq connector, \ce \cite{tassiElpiExtensionLanguage2018}. We will use \ce to implement the \elpi variant of \coqi{iIntros}, named above as \coqi{eiIntros}.

Our \elpi implementation \coqi{eiIntros} consists of three parts as seen in \cref{fig:eiintrosstruct}. The first two parts will interpret the DSL used to describe what we want to introduce. Then, the last part will apply the interpreted DSL. In \cref{ssec:tokenizer} we describe how a string is tokenized by the tokenizer. In \cref{ssec:parser} we describe how a list of tokens is parsed into a list of intro patterns. In \cref{ssec:applier} we describe how we use an intro pattern to introduce and eliminate the needed connectives. In every section we describe more parts of the \elpi programming language and the \ce connector starting with the base concepts of the language and working up to the mayor concepts of \elpi and \ce.
\begin{figure}
  \centering
  \begin{tikzpicture}[
      node distance=1cm and 2cm,
      >=stealth,
      auto
    ]
    \node[] (s) {};
    \node[basic box = green, below=of s] (t) {Tokenizer};
    \node[basic box = green, below=of t] (p) {Parser};
    \node[basic box = blue, below=of p] (id) {Introducer \& Destructor};
    \node[left=of id] (g) {};
    \node[below=of id] (e) {};

    \path[->,thick]
    (s) edge node[right]{\elpii{string}} (t)
    (t) edge node[right]{\elpii{list token}} (p)
    (p) edge node[right]{\elpii{list intro_pat}} (id)
    (g) edge node[above]{\elpii{goal}} (id)
    (id) edge node[right]{\elpii{proof, list goal}} (e);
  \end{tikzpicture}
  \caption{Structure of \coqi{eiIntros} with the input and output types on the edges.}
  \label{fig:eiintrosstruct}
\end{figure}
%Zeg dat ook het doel is om een tutorial te geven over elpi

\section{Tokenizer}\label{ssec:tokenizer}
The tokenizer takes as input a string. We will interpret every symbol in the string and produce a list of tokens from this string. Thus, the first step is to define our tokens. Next we show how to define a predicate that transform our string into the tokens we defined.

\subsection{Data types}\label{sssec:datatypes}
We have separated the introduction patterns into several distinct tokens. Most tokens just represent one or two characters, but some tokens also contain some data associated with that token. For example \elpii{"H1"} is tokenized as the name token containing the string "H1".
\begin{elpicode}
  kind token type.

  type tAnon, tFrame, tBar, tBracketL, tBracketR, tAmp,
       tParenL, tParenR, tBraceL, tBraceR, tSimpl,
       tDone, tForall, tAll token.
  type tName string -> token.
  type tNat int -> token.
  type tPure option string -> token.
  type tArrow direction -> token.

  kind direction type.
  type left, right direction.
\end{elpicode}
We first define a new type called token using the \elpii{kind} keyword, where \elpii{type} specifies the kind of our new type. Then we define several constructors for the token type. These constructors are defined using the \elpii{type} keyword, we specify a list of names for the constructors and then the type of those constructors. The first set of constructors do not take any arguments, thus have type \elpii{token}, and just represent one or more constant characters. The next few constructors take an argument and produce a token, thus allowing us to store data in the tokens. For example, \elpii{tName} has type \elpii{string -> token}, thus containing a string. Besides \elpii{string}, there are a few more basic types in \elpi such as \elpii{int}, \elpii{float} and \elpii{bool}. We also have higher order types, like \elpii{option A}, and later on \elpii{list A}.
\begin{elpicode}
  kind option type -> type.
  type none option A.
  type some A -> option A.
\end{elpicode}
Creating types of kind \elpii{type -> type} can be done using the \elpii{kind} directive and passing in a more complicated kind.

We can now represent a string as a list of these tokens. Given the string \elpii{"[H %H']"} we can represent it as the following list of type \elpii{token}: 
\begin{elpicode}
  [tBracketL, tName "H", tPure (some "H'"), tBracketR]
\end{elpicode}

\subsection{Predicates} \label{sssec:predicates}
Programs in \elpi consist of predicates. Every predicate can have several rules to describe the relation between its inputs and outputs.
\begin{elpicode}
  pred tokenize i:string, o:list token.
  tokenize S O :- 
    rex.split "" S SS,
    tokenize.rec SS O.
\end{elpicode}
Line 1 describes the type of the predicate. The keyword \elpii{pred} starts the definition of a predicate. Next we give the name of the predicate, "tokenize". Lastly, we give a list of arguments of our predicate. Each argument is marked as either \elpii{i:}, they act as an input or \elpii{o:}, they act as an output, in \cref*{sssec:mandu} a more precise definition is given. In the only rule of our predicate, defined on line 2, we assign a variable to both of the arguments. \elpii{S} has type \elpii{string} and is bound to the first argument. \elpii{O} has type \elpii{list token} and is bound to the second argument. By calling predicates after the \elpii{:-} symbol we can define the relation between the arguments. The first predicate we call, \elpii{rex.split}, has the following type:
\begin{elpicode}
  pred rex.split i:string, i:string, o:list string.
\end{elpicode}
When we call it, we assign the empty string to its first argument, the string we want to tokenize to the second argument, and we store the output list of string in the new variable \elpii{SS}. This predicate allows us to split a string at a certain delimiter. We take as delimiter the empty string, thus splitting the string up in a list of strings of one character each. Strings in \elpi are based on OCaml strings and are not lists of characters. Since \elpi does not support pattern matching on partial strings, we need this workaround.

The next line, line 4, calls the recursive tokenizer, \elpii{tokenizer.rec}\footnote{Names in \elpi can have special characters in them like \elpii{.}, \elpii{-} and \elpii{>}, thus, \elpii{tokenize} and \elpii{tokenize.rec} are fully separate predicates. It is just a convention that when creating a helper predicate we name it by adding a dot and a short name for the helper.}, on the list of split string and assigns the output to the output variable \elpii{O}.

The reason predicates in \elpi are called predicates and not functions, is that they don't always have to take an input and give an output. They can sometimes better be seen as predicates defining for which values of their arguments they hold. Each rule defines a list of predicates that need to hold for their premise to hold. Thus, a predicate can have multiple values for its output, as long as they hold for all contained rules. These multiple possible values can be reached by backtracking, which we will discuss in \cref*{sssec:backtracking}. To execute a predicate, we thus find the first rule for which its premise is sufficient for the arguments we supply. We then check if each of the predicates in the conclusion hold starting at the top. If they hold, and we get a value for every output argument, we are done executing our predicate. How we determine when arguments are sufficient and what happens when a rule does not hold, we will discuss in the next two sections.

\subsection{Matching and unification}\label{sssec:mandu}
The arguments of a predicate can be more than just a variable. We can supply a value containing variables and depending on the argument mode, input or output, we match or unify the input with the premise respectively.

\elpii{tokenize.rec} uses matching and unification to solve most cases.
\begin{elpicode}
  pred tokenize.rec i:list string, o:list token.
  tokenize.rec [] [] :- !.
  tokenize.rec [" " | SL] TS :- !, tokenize.rec SL TS.
  tokenize.rec ["$" | SL] [tFrame | TS] :- !, 
    tokenize.rec SL TS.
  tokenize.rec ["/", "/", "=" | SL] [tSimpl, tDone | TS] :- !, 
    tokenize.rec SL TS.
  tokenize.rec ["/", "/" | SL] [tDone | TS] :- !, 
    tokenize.rec SL TS.
\end{elpicode}
This predicate has several rules, we chose a few to highlight here. The first rule, on line 2, has a premise and a cut as its conclusion, we will discuss cuts in \cref*{sssec:backtracking}, for now they can be ignored. This rule can be used when the first argument matches \elpii{[]} and if the second argument unifies with \elpii{[]}. The difference is that, for two values to match they must have the exact same constructors and can only contain variables in the same places in the value. Thus, the only valid value for the first argument of the first rule is \elpii{[]}. When unifying two values we allow a variable to be unified with a constructor, when this happens the variable will get assigned the value of the constructor. Thus, we can either pass \elpii{[]} to the second argument, or some variable \elpii{V}. After the execution of the rule the variable \elpii{V} will have the value \elpii{[]}.

The next four rules use the same principle. They use the list pattern \elpii{[E1, ..., En | TL]}, where \elpii{E1} to \elpii{En} are the first $n$ values and \elpii{TL} is the rest of the list, to match on the first few elements of the list. We unify the output with a list starting with the token that corresponds to the string we match on. The tails of the input and output we pass to the recursive call of the predicate to solve.

When we encounter multiple rules that all match the arguments of a rule we try the first one first. The rules on line 6 and 8 would both match the value \elpii{["/", "/", "="]} as first argument. But, we interpret this use the rule on line 6 since it is before the rule on line 8. This results in our list of strings being tokenized as \elpii{[tSimpl, tDone]}.

A fun side effect of output being just variables we pass to a predicate is that we can also easily create a function that is reversible. If we change the mode of our first argument to output and move rule 3 to the bottom, we can pass in a list of tokens and get back a list of strings representing this list of tokens.
\quest{Don't know what to do with this, but is an interesting fact and shows the versatility, we might use it later.}

\subsection{Functional programming in \elpi}
While our language is based on predicates we still often defer to a functional style of programming. The first language feature that is very useful for this goal is spilling. Spilling allows us to write the entry point of the tokenizer as defined in \cref*{sssec:predicates} without the need of the temporary variable to pass the list of strings around.
\begin{elpicode}
  pred tokenize i:string, o:list token.
  tokenize S O :- tokenize.rec {rex.split "" S} O.
\end{elpicode}

We spill the output of a predicate into the input of another predicate by using the \elpii{{ }} syntax. We don't specify the last argument of the predicate and only the last argument of a predicate can be spilled. It is mostly equal to the previous version, but just written shorter. There is one caveat, but it will be discussed in ?.
\todoo{Refer to relevant section}

The second useful feature is how lambda expressions are first class citizens of the language. A \elpii{pred} statement is a wrapper around a constructor definition using the keyword \elpii{type}, where all arguments are in output mode. The following predicate is equal to the type definition below it.
\begin{elpicode}
  pred tokenize i:string, o:list token.
  type tokenize string -> list token -> prop.
\end{elpicode}
The \elpii{prop} type is the type of propositions, and with arguments they become predicates. We are thus able to write predicates that accept other predicates as arguments.
\begin{elpicode}
  pred map i:list A, i:(A -> B -> prop), o:list B.
  map [] _ [].
  map [X|XS] F [Y|YS] :- F X Y, map XS F YS.
\end{elpicode}
\elpii{map} takes as its second argument a predicate on \elpii{A} and \elpii{B}. On line 3 we map this predicate to the variable \elpii{F}, and we then use it to either find a \elpii{Y} such that \elpii{F X Y} holds, or check if for a given \elpii{Y}, \elpii{F X Y} holds. We can use the same strategy to implement many of the common functional programming higher order functions.

\subsection{Backtracking} \label{sssec:backtracking}
In this section we will finally describe what happens when a rule fails to complete halfway through. We start with a predicate which will be of much use for the last part of our tokenizer.
\begin{elpicode}
  pred take-while-split i:list A, i:(A -> prop), 
                        o:list A, o:list A.
  take-while-split [X|XS] Pred [X|YS] ZS :- Pred X,
    take-while-split XS Pred YS ZS.
  take-while-split XS _ [] XS.
\end{elpicode}
\elpii{take-while-split} is a predicate that should take elements of its input list till its input predicate no longer holds and then output the first part of input in its third argument and the last part of the input in its fourth argument.

The predicate contains two rules. The first rule, defined on lines 2 and 3, recurses as long as the input predicate, \elpii{Pred} holds for the input list, \elpii{[X|XS]}. The second rule returns the last part of the list as soon as \elpii{Pred} no longer holds.

The first rule destructs the input in its head \elpii{X} and its tail \elpii{XS}. It then checks if \elpii{Pred} holds for \elpii{X}, if it does, we continue the rule and call \elpii{take-while-split} on the tail while assigning X as the first element of the first output list and the output of the recursive call as the tail of the first output and the second output. However, if \elpii{Pred X} does not succeed we backtrack to the previous rule in our conclusion. Since there is no previous rule in the conclusion we instead undo any unification that has happened and try the next possible rule. This will be the rule on line 4 and returns the input as the second output of the predicate.

We can use \elpii{take-while-split} to define the rule for the token \elpii{tName}
\begin{elpicode}
  type tName string -> token.

  tokenize.rec SL [tName S | TS] :-
    take-while-split SL is-identifier S' SL',
    { std.length S' } > 0, !,
    std.string.concat "" S' S,
    tokenize.rec SL' TS.
\end{elpicode}
To tokenize a name we first call \elpii{take-while-split} with as predicate \elpii{is-identifier}, which checks if a string is valid identifier character, whether it is either a letter or one of a few symbols allowed in identifiers. It thus splits up the input string list into a list of string that is a valid identifier and the rest of the input.
On line 5 we check if the length of the identifier is larger than 0. We do this by spilling the length of \elpii{S'} into the \elpii{>} predicate.
Next, on line 6, we concatenate the list of strings into one string, which will be our name.
And on line 7, we call the tokenizer on the rest of the input, to create the rest of our tokens.

If our length check does not succeed we backtrack to next rule that matches, which is
\begin{elpicode}
  tokenize.rec XS _ :- !, 
    coq.say "unrecognized tokens" XS, fail.  
\end{elpicode}
It prints an error messages saying that the input was not recognized as a valid token, after which it fails. The predicate thus does not succeed. There is one problem, if line 6 or 7 fails for some reason in the \elpii{tName} rule of the tokenizer, the current input starting at \elpii{X} is not unrecognized as we managed to find a token for the name at the start of the input. Thus, we don't want to backtrack to another rule of \elpii{tokenize.rec} when we have found a valid name token. This is where the cut symbol, \elpii{!}, comes in. It cuts the backtracking and makes certain that if we fail beyond that point we don't backtrack in this predicate.

If we take the following example
\begin{elpicode}
  tokenize.rec ["H","^"] TS
              ~$\Downarrow \text{calls}$~ 
  tokenize.rec ["^"] TS'
\end{elpicode}
When evaluating this predicate we would first apply the name rule of the \elpii{tokenize.rec} predicate. This would unify \elpii{TS} with \elpii{[tName "H" | TS']} and call line 3, \elpii{tokenize.rec ["^"] TS'}. Every rule of \elpii{tokenize.rec} fails including the last fail rule. This rule does first print \elpii{"unrecognized tokens ^"} but then also fails. Now when executing the rule of line 1, we have failed on the last predicate of the rule. If there was no cut before it, we would backtrack to the fail rule and also print \elpii{"unrecognized tokens [H, ^]"}. But, because there is a cut we don't print the faulty error message. Thus, we only print meaningful error message when we fail to tokenize an input.

\section{Parser}\label{ssec:parser}
\quest{Ik twijfel over dit hele stuk aangezien er niet zo veel nieuws in wordt uitgelegt dat nuttig is voor later, behalve pas op met backtracking.}
\begin{itemize}
  \item Describe sections of parser
\end{itemize}
Alternative for this section
\begin{itemize}
  \item Parser uses many of the same techniques as tokenizer for parsing
  \item Not much to explain
  \item Implements a reductive descent parsing
  \item Minimize backtracking
  \item Look at code for full details
\end{itemize}

\subsection{Data structure}
\begin{itemize}
  \item structured to be easily read to apply the intro pattern.
\end{itemize}
\begin{elpicode}
  kind ident type.
  type iNamed string -> ident.
  type iAnon term -> ident.

  kind intro_pat type.
  type iFresh, iSimpl, iDone intro_pat.
  type iIdent ident -> intro_pat.
  type iList list (list intro_pat) -> intro_pat.
\end{elpicode}
\begin{itemize}
  \item Tree structure?
  \item iList is combination of existential, disjunction and conjunction pattern
\end{itemize}

\subsection{Reductive descent parsing}
\begin{itemize}
  \item We can translate a grammar directly into a parser
  \item Below, partial grammar for the intro patterns
\end{itemize}

\begin{grammar}
  <intropattern\_list> ::= $\epsilon$
  \alt <intropattern> <intropattern\_list>

  <intropattern> ::= <ident>
  \alt `?' | `/=' | `//'
  \alt `[' <intropattern\_list> `]'
  \alt `(' <intropattern\_conj\_list> `)'

  <intropattern\_list> ::= $\epsilon$
  \alt <intropattern> `|' <intropattern\_list>
  \alt <intropattern> <intropattern\_list>

  <intropattern\_conj\_list> ::= $\epsilon$
  \alt <intropattern> `&' <intropattern\_conj\_list>
\end{grammar}

\begin{itemize}
  \item Explain structure of parser
  \item give example of anon, simpl and done
  \item Using tokenizer name has become the same
\end{itemize}

\begin{elpicode}
  pred parse_ip i:list token, o:list token, o:intro_pat.
  parse_ip [tAnon | TS] TS (iFresh) :- !.
  parse_ip [tSimpl | TS] TS (iSimpl) :- !.
  parse_ip [tDone | TS] TS (iDone) :- !.
  parse_ip [tName X | TS] TS (iIdent (iNamed X)) :- !.
\end{elpicode}

\begin{itemize}
  \item Check after calling new parser that conditions for values hold
  \item Post process conj parser result
\end{itemize}

\begin{elpicode}
  parse_ip [tBracketL | TS] TS' (iList L) :- !,
  parse_ilist TS [tBracketR | TS'] L.
  parse_ip [tParenL | TS] TS' IP :- !,
  parse_conj_ilist TS [tParenR | TS'] L',
  {std.length L'} >= 2,
  foldr {std.drop-last 2 L'} (iList [{std.take-last 2 L'}]) (x\ a\ r\ r = iList [[x, a]]) IP.
\end{elpicode}
\begin{itemize}
  \item Recursive parser
\end{itemize}

\begin{elpicode}
  pred parse_ilist i:list token, o:list token, o:list (list intro_pat).
  parse_ilist [tBracketR | TS] [tBracketR | TS] [[]].
  parse_ilist TS R [[IP] | LL'] :-
    parse_ip TS [tBar | RT] IP,
    parse_ilist RT R LL'.
  parse_ilist TS R [[IP | L] | LL'] :-
    parse_ip TS RT IP,
    parse_ilist RT R [L | LL'].

  pred parse_conj_ilist i:list token, o:list token, o:list intro_pat.
  parse_conj_ilist TS [tParenR | R] [IP] :- 
    parse_ip TS [tParenR | R] IP.
  parse_conj_ilist TS R [IP | L'] :-
    parse_ip TS [tAmp | RT] IP,
    parse_conj_ilist RT R L'.
\end{elpicode}

\subsection{Danger of backtracking}
\begin{itemize}
  \item Show timing of current \elpii{parse_ilist} code on larger inputs
  \item Change backtracking
  \item Show new timings
  \item Explain why it is better
\end{itemize}

\begin{elpicode}
  pred parse_ilist i:list token, o:list token, o:list (list intro_pat).
  parse_ilist [tBracketR | TS] [tBracketR | TS] [[]].
  parse_ilist TS R [IPS | LL'] :-
    parse_ip TS RT IP,
    (
      (
        RT = [tBar | RT'],
        parse_ilist RT' R LL',
        IPS = [IP]
      );
      (
        parse_ilist RT R [L | LL'],
        IPS = [IP | L]
      )
    ).
\end{elpicode}

\section{Applier}\label{ssec:applier}
\begin{itemize}
  \item Only used standard \elpi
  \item Now use \ce
  \item What \ce adds
  \item Section overview
\end{itemize}

\subsection{Elpi coq HOAS}
\begin{itemize}
  \item First step, represent \coq terms in \elpi
  \item Names and function application are just constructors
\end{itemize}
\coqi{1+1}
\begin{elpicode}
  app [global (const «Nat.add»), 
      app [global (indc «S»), global (indc «O»)], 
      app [global (indc «S»), global (indc «O»)]]
\end{elpicode}
\begin{itemize}
  \item Explain app, global, const, indc and «»
\end{itemize}

\begin{itemize}
  \item \ce uses higher-order abstract syntax (HOAS)
  \item functions in \coq are functions that produce terms in \ce
\end{itemize}
\coqi{fun (n: nat), n + 1}
\begin{elpicode}
  FUN = fun `n` (global (indt «nat»)) n \ 
          app [global (indt «sum»), 
              n, 
              app [global (indc «S»), global (indc «O»)]]
\end{elpicode}
\begin{itemize}
  \item fun constructor taking name, type and function producing term
  \item footnote about names all being convertible
\end{itemize}
\begin{elpicode}
  type fun  name -> term -> (term -> term) -> term.
\end{elpicode}
\begin{itemize}
  \item prod, let, fix work the same
\end{itemize}

\subsection{Coq context in Elpi}
\begin{itemize}
  \item Looking at terms in functions becomes hard as we need to give the function an input to get the term
  \item introduce fresh constant using \elpii{pi x\ }
\end{itemize}
\begin{elpicode}
  FUN = fun _ _ F,
  pi x\ F x = app [_, _, P],
  P = app [global (indc «S»), global (indc «O»)]
\end{elpicode}
\begin{itemize}
  \item Take function out of constructor
  \item Fill in function with existential variable to inspect contents
  \item Take out number we add
  \item We lose type and name information about x
\end{itemize}
\begin{elpicode}
  pred decl i:term, o:name, o:term.
  decl x `n` (global (indt «nat»)).
\end{elpicode}
\begin{itemize}
  \item decl rule describes types and names of variables
  \item Lookup type using \elpii{decl x N T}
  \item We have to add the rule when we define x
\end{itemize}
\begin{elpicode}
  pi x\ decl x `n` (global (indt «nat»)) 
          => coq.typecheck (F x) Type ok.
\end{elpicode}
\begin{itemize}
  \item We add a rule to the top of the rules for the execution of the code after the \elpii{=>}
  \item During typechecking, \elpii{decl x N T} is executed resulting in ...
  \item \elpii{Type} becomes (global (indt «nat»))
  \item \elpii{=>} has many more uses later on
\end{itemize}

\subsection{Quotation and anti-quotation}
\begin{itemize}
  \item Writing terms is a lot of work
  \item \ce allows us to write \coq code that is translated immediately using imports in current file
\end{itemize}
\begin{elpicode}
  {{ λ (n: nat), n + 1 }} =
    fun `n` (global (indt «nat»)) c0 \ 
        app [global (indt «sum»), 
            c0, 
            app [global (indc «S»), global (indc «O»)]]
\end{elpicode}
\begin{itemize}
  \item \ce also allows putting \elpi vars in \coq terms (anti quotation)
\end{itemize}
\begin{elpicode}
  {{ @envs_entails lp:PROP (@Envs lp:PROPE lp:CI lp:CS lp:N) lp:P }} 
\end{elpicode}
\begin{itemize}
  \item Extract values from term
  \item Insert values in term, useful in proofs
\end{itemize}
\begin{elpicode}
  {{ as_emp_valid_2 lp:Type _ (tac_start _ _) }}
\end{elpicode}
\begin{itemize}
  \item Lemma useful in next section
  \item Type is type of goal we want to proof
  \item Term becomes lemma we can apply to goal
\end{itemize}
\subsection{Proofs in Elpi}
\begin{itemize}
  \item Proofs in \elpi built up proof term step by step
  \item Pass around Type of goal and variable to assign proof term to
  \item This is hole
\end{itemize}
\begin{elpicode}
  kind hole type.
  type hole term -> term -> hole. % hole Type Proof
\end{elpicode}
\begin{itemize}
  \item Proofs take a hole and often produce new holes
  \item Following proof step applies the ex-Falso proof step
  \item Replace type with False
\end{itemize}
\begin{elpicode}
  pred do-iExFalso i:hole, o:hole.
  do-iExFalso (hole Type Proof) (hole FalseType FalseProof) :-
    coq.elaborate-skeleton {{ tac_ex_falso _ _ _ }} Type Proof ok,
    Proof = {{ tac_ex_falso _ _ lp:FalseProof }},
    coq.typecheck FalseProof FalseType ok.
\end{elpicode}
\begin{coqcode}
  Lemma tac_ex_falso Δ Q : envs_entails Δ False → envs_entails Δ Q.
\end{coqcode}
\begin{itemize}
  \item Elaborate Lemma against type to generate proof term will be Lemma filled in with necessary values
  \item Next, extract New proof variable
  \item Get type of new proof variable
\end{itemize}

\subsubsection{Iris context counter}
\begin{itemize}
  \item Iris can have anonymous hypotheses in context
  \item Keep track of number to assign to anon hypothesis
  \item Normally in Type
  \item Since we derive the type from the proof term we have to apply increases in this number in the proof term
  \item Instead we keep track of it separately
\end{itemize}
\begin{elpicode}
  pred do-iStartProof i:hole, o:ihole.
  do-iStartProof (hole Type Proof) (ihole N (hole NType NProof)) :- 
    coq.elaborate-skeleton {{ as_emp_valid_2 lp:Type _ (tac_start _ _) }} Type Proof ok,
    Proof = {{ as_emp_valid_2 _ _ (tac_start _ lp:NProof) }},
    coq.typecheck NProof NType ok,
    NType = {{ envs_entails (Envs _ _ lp:N) _}}.
\end{elpicode}
\begin{itemize}
  \item Start proof applies start proof lemma
  \item Next extracts current anon hypotheses count
  \item Stores it in hole using new type ihole
\end{itemize}
\begin{elpicode}
  kind ihole type.
  type ihole term -> hole -> ihole. % ihole iris hyp counter, (hole type proof)
\end{elpicode}
\begin{itemize}
  \item Counter is \coq positive since increasing it is fairly easy
\end{itemize}
\begin{elpicode}
  pred increase-ctx-count i:term, o:term.
  increase-ctx-count N NS :-
    coq.reduction.vm.norm {{ Pos.succ lp:N }} _ NS.
\end{elpicode}
\begin{itemize}
  \item We can increase counter and put it in the resulting \elpii{ihole} when necessary.
\end{itemize}

\subsection{Continuation Passing Style}
\begin{itemize}
  \item When introducing a forall we need to add the variable to our context
  \item Next steps in the proof thus need the new value in the context
  \item We have to use continuation passing style
\end{itemize}
\begin{elpicode}
  pred do-intro-anon i:hole, i:(hole -> prop).
  do-intro-anon (hole Type Proof) C :-
    coq.ltac.fresh-id "a" {{ False }} ID,
    coq.id->name ID N,
    coq.elaborate-skeleton (fun N _ _) Type Proof ok,
    Proof = (fun _ T IntroFProof),
    @pi-decl N T x\ 
      coq.typecheck (IntroFProof x) (F x) ok,
      C (hole (F x) (IntroFProof x)).
\end{elpicode}
\begin{itemize}
  \item This introduces a variable without needing a name
  \item first two steps create the name of the variable
  \item Next we use a function as the proof term
  \item We extract the (term -> term) proof variable and the type
  \item Add the new variable to the context with the name
  \item Get the type of the new hole
  \item Call the continuation function on the hole in the context
\end{itemize}

\begin{itemize}
  \item In our eiIntros tactic we will be calling predicates like \elpii{do-intro-anon} and thus we get a similar type
\end{itemize}
\begin{elpicode}
  pred do-iIntros i:(list intro_pat), i:ihole, i:(ihole -> prop).
  do-iIntros [] IH C :- !, C IH.
  do-iIntros [iPure (none) | IPS] (ihole N (hole Type Proof)) C :-
    coq.elaborate-skeleton {{ tac_forall_intro_nameless _ _ _ _ _ _ }} Type Proof ok,
    Proof = {{ tac_forall_intro_nameless _ _ _ _ _ lp:IProof }},
    coq.typecheck IProof IType ok, !,
    do-intro-anon (hole IType IProof) (h\ sigma IntroProof\ sigma IntroType\ sigma NormType\
      h = hole IntroType IntroProof,
      coq.reduction.lazy.bi-norm IntroType NormType, !,
      do-iIntros IPS (ihole N (hole NormType IntroProof)) C
    ).
\end{elpicode}
\begin{itemize}
  \item The predicate \elpii{do-iIntros} gets a list of intro patterns, an ihole and the continuation function
  \item Base case calls the cont. predicate
  \item Pure intro case
  \item First transform goal to put forall at the top of goal
  \item Then use \elpii{do-intro-anon} to introduce that variable
  \item Lastly normalize the type and call iIntros on the new hole
  \item No anon \iris hypotheses introduced thus counter stays the same
\end{itemize}

\subsection{Backtracking in proofs}
\quest{We don't actually need to backtrack here, we can just look at the type and see which case we need}
\begin{elpicode}
  pred do-iIntro-ident i:ident, i:ihole, o:ihole.
  do-iIntro-ident ID (ihole N (hole Type Proof)) 
                     (ihole N (hole IType IProof)) :-
    ident->term ID _ T,
    coq.elaborate-skeleton 
      {{ tac_impl_intro _ lp:T _ _ _ _ _ _ _ }} 
      Type Proof ok, !,
    Proof = 
      {{ tac_impl_intro _ _ _ _ _ _ _ _ lp:IProof }},
    coq.typecheck IProof IType' ok,
    pm-reduce IType' IType,
    if (IType = {{ False }}) 
       (coq.error "eiIntro: " X " not fresh")
       (true).
  do-iIntro-ident ID (ihole N (hole Type Proof)) 
                     (ihole N (hole IType IProof)) :-
    ident->term ID _ T,
    coq.elaborate-skeleton 
      {{ tac_wand_intro _ lp:T _ _ _ _ _ }} 
      Type Proof ok, !,
    Proof = {{ tac_wand_intro _ _ _ _ _ _ lp:IProof }}, 
    coq.typecheck IProof IType' ok,
    pm-reduce IType' IType,
    if (IType = {{ False }}) 
       (coq.error "eiIntro: " X " not fresh") 
       (true).
  do-iIntro-ident ID _ _ :-
    ident->term ID X _,
    coq.error "eiIntro: " X " could not introduce".
\end{elpicode}

\subsection{Starting the tactic}
\begin{itemize}
  \item Solve is the entry point
  \item Gets a goal with type proof and the arguments
\end{itemize}
\begin{elpicode}
  solve (goal _ _ Type Proof [str Args]) GS :-
    tokenize Args T, !,
    parse_ipl T IPS, !,
    do-iStartProof (hole Type Proof) IH, !,
    do-iIntros IPS IH (ih\ set-ctx-count-proof ih _), !,
    coq.ltac.collect-goals Proof GL SG,
    all (open pm-reduce-goal) GL GL',
    std.append GL' SG GS.
\end{elpicode}
\begin{itemize}
  \item First we parse the arguments
  \item Ten start proof and get the ihole
  \item Then start do-iIntros where at the end we put the context counter in the proof
  \item ...
  \item ...
\end{itemize}

\section{Writing commands}

\end{document}